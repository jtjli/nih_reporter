# Databricks notebook source
# MAGIC %md
# MAGIC
# MAGIC Downloaded data can be copied to databricks using databricks-cli to avoid dropouts via web browser.
# MAGIC
# MAGIC ```
# MAGIC tar czvf nih.tgz raw_data
# MAGIC databricks fs cp nih.tgz 'dbfs:/FileStore/nih/'
# MAGIC ```
# MAGIC
# MAGIC Then use the following codes to untar and check dir content.

# COMMAND ----------

display(dbutils.fs.ls('/FileStore/nih'))

# COMMAND ----------

# MAGIC %sh
# MAGIC cd /dbfs/FileStore/nih
# MAGIC #cd /dbfs/FileStore/nih/raw_data/downloaded_230325/Projects
# MAGIC #rm -rf raw_data
# MAGIC ls -l

# COMMAND ----------

# MAGIC %sh
# MAGIC cd /dbfs/FileStore/nih
# MAGIC ls -l
# MAGIC tar xzvf nih.tgz --no-same-owner

# COMMAND ----------

# MAGIC %sh
# MAGIC cd /dbfs/FileStore/nih
# MAGIC
# MAGIC # Clean up unwanted files generated by MacOS (eg ._DSStore etc)
# MAGIC find . -name ".*" | xargs rm

# COMMAND ----------

display(dbutils.fs.ls('/FileStore/nih/raw_data/downloaded_230325'))
display(dbutils.fs.ls('/FileStore/nih/raw_data/downloaded_230325/Projects'))
display(dbutils.fs.ls('/FileStore/nih/raw_data/downloaded_230325/Abstracts'))
display(dbutils.fs.ls('/FileStore/nih/raw_data/downloaded_230325/LinkTables'))
display(dbutils.fs.ls('/FileStore/nih/raw_data/downloaded_230325/Publications'))


# COMMAND ----------


# UNZIP
# Learnt the hard way - don't deal with zip file directly with spark (gzip maybe ok). Compatibility between spark and normal pandas (non pyspark pandas) seems problematic
import zipfile
data_root = '/FileStore/nih/raw_data/downloaded_230325'
folders = ['Projects','Abstracts','LinkTables','Publications']

for folder in folders:
    ffp = f"{data_root}/{folder}"
    
    all_zips = [x[1] for x in dbutils.fs.ls(ffp)]
    for f in all_zips:
        zffp = f"/dbfs{ffp}/{f}"
        zf = zipfile.ZipFile(zffp)
        csvf = zf.namelist()
        if len(csvf) > 1:
            print (f"SKIPPING {f} because it contains >1 csv files. N = : {len(csvf)}")
            continue

        assert len(csvf) == 1, f"{f} {csvf}"
        csvf = csvf[0]
        ffp_out = "/dbfs" + ffp + "_unzipped"
        print(f"Unzipping {csvf} into {ffp_out}")
        zf.extractall(ffp_out)

# with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:
#     zip_ref.extractall(directory_to_extract_to)

# COMMAND ----------

display(dbutils.fs.ls('/FileStore/nih/raw_data/downloaded_230325'))
display(dbutils.fs.ls('/FileStore/nih/raw_data/downloaded_230325/Projects_unzipped'))
display(dbutils.fs.ls('/FileStore/nih/raw_data/downloaded_230325/Abstracts_unzipped'))
display(dbutils.fs.ls('/FileStore/nih/raw_data/downloaded_230325/LinkTables_unzipped'))
display(dbutils.fs.ls('/FileStore/nih/raw_data/downloaded_230325/Publications_unzipped'))
